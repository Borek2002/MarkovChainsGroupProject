{
  "teoria": "Theory",
  "grafy": "Graph",
  "macierze": "Matrices",
  "header": "Visualization of Markov Chains",
  "macierzIWektor": "Matrix and Vector",
  "iloscKolumnIWierszy": "Number of states of the Markov chain",
  "macierz": "Transition matrix",
  "wektor": "Vector",
  "obliczPrawdopodobienstwoFinalne": "Calculate final probability",
  "obliczPrawdopodobieństwoPoNKrokach": "Calculate probability after n steps",
  "obliczPrawdopodobieństwoStacjonarne": "Calculate stationary probability",
  "stanPochlaniajacy": "Immersive state",
  "Zapisz": "Save",
  "matrix_tooltip": "Transition Matrix in Markov Chain is a two-dimensional array where each element represents the transition probabilities from one state to another. Each row of the matrix corresponds to the initial state, and each column to the destination state. The value in a particular element tells us the probability of transitioning from the initial state to the destination state.",
  "initial_vector_tooltip": "The initial vector in a Markov chain is a one-dimensional set of values that represents the initial probabilities of the occurrence of each state in the chain. Each element of this vector corresponds to one of the states in the Markov chain. By multiplying this vector by the transition matrix, we obtain a new vector that represents the probabilities of the states occurring in the next time step.",
  "rowsAndCols_description": "The number of columns in the transition matrix of a Markov chain corresponds to the number of possible states that the process can assume. Each column represents one of these states. Similarly, the number of rows also corresponds to the number of states. The number of columns and rows in the transition matrix describes the structure of the Markov chain, where columns represent the states, and rows represent the transition probabilities between them.",
  "transition_probabilities_sum_description": "The sum of transition probabilities for each state (i.e., for each row in the transition matrix) must equal 1. This means that each state has assigned probabilities of transitioning to other states, and the sum of these probabilities for any given state always equals 1.",
  "markov_chains" : "Markov chains",
  "introduction_title": "Introduction to Markov Chains",
  "markov_chains_introduction": "Markov chains are stochastic processes in which the future state of the system depends only on the current state, not on the entire history of the process. Initially used for modeling random processes, they have become a popular tool in various fields such as computer science, economics, biology, etc.",
  "basic_concepts_title": "Basic Concepts",
  "state": "State:",
  "transition_matrix": "Transition Matrix:",
  "transition_probability": "Transition Probability:",
  "state_definition": "Each possible outcome of a process.",
  "transition_matrix_definition": "A matrix specifying the probabilities of transitions between states.",
  "transition_probability_definition": "The probability of transitioning from one state to another.",
  "markov_properties_title": "Properties of Markov Chains",
  "ergodicity": "Ergodicity:",
  "ergodicity_description": "A Markov chain is ergodic if it is possible to reach any other state from any state in a finite amount of time.",
  "stationarity": "Stationarity:",
  "stationarity_description": "A Markov chain reaches a stationary state when the probability distribution does not change over time.",
  "applications_examples" : "Applications examples",
  "finance" : "Finance",
  "social_sciences" : "Social sciences",
  "biology": "Biology",
  "engineering": "Engineering",
  "computer_science": "Computer science",
  "telecommunications": "Telecommunications",
  "finance_description" : "Markov chains are used for modeling and forecasting financial markets, such as the stock market. They can be applied to analyze stock price volatility, forecast future prices, and assess investment risk.",
  "social_sciences_description": "In sociology and behavioral economics, Markov chains can be used to model social behaviors such as political elections, consumer preferences, and labor market dynamics.",
  "biology_description": "Markov chains are used to model various biological processes such as genetic evolution, life cycles of organisms, and population dynamics.",
  "engineering_description": "In systems engineering and production management, Markov chains can be used for modeling and optimizing production processes, inventory management, route planning in transportation, and reliability analysis of systems.",
  "computer_science_description": "In computer science, Markov chains are used in modeling and analyzing search algorithms, network traffic analysis, resource planning in computer systems, and many other applications.",
  "telecommunications_description": "In telecommunications networks, Markov chains can be used for modeling traffic, analyzing data flow, capacity planning, and optimizing network performance.",
  "markov_visualization_title": "Markov Chain Visualization",
  "weather_relations_description": "The following diagram illustrates the relationships between different weather states, such as sunny, cloudy, and rainy, also taking into account the transition probabilities between them, expressed in percentages.",
  "fractional_relations_description": "The following diagram illustrates the relationships between different states S1,S2,S3,S4,S5, taking into account the transition probabilities between them, expressed fractionally.",
  "transition_matrix_description": "The transition matrix describing the Markov chain looks as follows:",
  "irreversible_state_description": "The irreversible state S1 occurs only once, with no possibility of returning to it.",
  "absorbing_state_description": "The absorbing state S3, once entered, remains in it until the end.",
  "summary_title": "Summary",
  "markov_chains_description": "Markov chains are powerful tools for modeling random processes, whose behavior depends on the current state. The characteristic property of Markov chains is that the future state of the process depends solely on the current state, not on the history of states, which makes them useful for modeling many real-life situations. Their applications are unlimited - from predicting stock prices on the stock exchange to modeling social behaviors in social sciences, and even analyzing traffic in telecommunications networks. Through the analysis of Markov chains, scientists, analysts, and engineers can draw valuable conclusions about the future states of processes, identify trends and patterns, and make more informed decisions.",
  "no_immersive_state": "No immersive state",
  "matrix_description": "This is a matrix that represents the transition probabilities between states in a Markov chain. The values in each cell should be numbers between 0 and 1. The sum of values in each row should be 1, which means that the probability of transitioning from a given state to all possible states is 100%.",
  "vector_description": "This is a vector representing the initial probability distribution of the states. Each value in the vector represents the probability that the Markov chain will start in a given state. The sum of the values in the vector should be 1.",
  "final_probability": "Final probability",
  "final_probability_description": "The probability that the Markov chain will be in a given state after an infinite number of steps. This is a state in which the probability distribution does not change despite further steps of the Markov chain. Formally, this is the vector π that satisfies the equation πP = π, where P is the transition matrix.",
  "probability_after_n_steps": "Probability after n steps",
  "probability_after_n_steps_description": "The probability distribution of the states of the Markov chain after n steps. The initial probability distribution is multiplied by the transition matrix P n times.",
  "stationary_probability": "Stationary probability",
  "stationary_probability_description": "The probability distribution that remains unchanged under the action of the transition matrix P. The stationary probability vector π satisfies the equation: πP = π and ∑πi = 1, where πi is the probability of being in state i in the stationary distribution.",
  "immersive_state_description": "An immersive state in the context of Markov chains is a state from which there is no exit - once reached, it remains unchanged. In the transition matrix, an immersive state has the probability of transitioning to itself equal to 1 and to all other states equal to 0."

}
